{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4fc8eb",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26826f67",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb944ce",
   "metadata": {},
   "source": [
    "*Explore the foundational concepts of deep learning, neural networks, and their transformative applications.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4acf8",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d8e5b",
   "metadata": {},
   "source": [
    "- Deep learning is revolutionizing various industries, from healthcare to self-driving cars, enabling new products and services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f6c04",
   "metadata": {},
   "source": [
    "- AI is considered the \"new electricity,\" poised to transform society as profoundly as electrification did a century ago."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b5f5a1",
   "metadata": {},
   "source": [
    "- Supervised learning is currently the most economically valuable application of neural networks, mapping inputs (x) to outputs (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56498a85",
   "metadata": {},
   "source": [
    "- A neural network is formed by stacking simple processing units, or neurons, into layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee1c11",
   "metadata": {},
   "source": [
    "- Different types of neural networks (CNNs, RNNs) are best suited for specific data types like images or sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600e5ee",
   "metadata": {},
   "source": [
    "- The recent surge in deep learning success is driven by three main factors: **data scale**, **computational power**, and **algorithmic innovation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d053e6",
   "metadata": {},
   "source": [
    "- Iterative development cycles, sped up by faster computation, are crucial for effective neural network development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdf7460",
   "metadata": {},
   "source": [
    "- Structured data (databases) and unstructured data (images, audio, text) both benefit significantly from deep learning advancements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c2ab1",
   "metadata": {},
   "source": [
    "- Trusting one's intuition and continuous programming are key advice from deep learning pioneers like Geoffrey Hinton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef25f5",
   "metadata": {},
   "source": [
    "## Concepts Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e326eda8",
   "metadata": {},
   "source": [
    "### What is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa151308",
   "metadata": {},
   "source": [
    "- Deep learning refers to training neural networks, often very large ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8c310",
   "metadata": {},
   "source": [
    "- It is a subset of machine learning that utilizes algorithms inspired by the structure and function of the human brain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14918e77",
   "metadata": {},
   "source": [
    "- **Impact**: Deep learning has transformed internet services (web search, advertising) and is enabling new applications in healthcare, personalized education, agriculture, and autonomous driving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69863d6",
   "metadata": {},
   "source": [
    "### The \"New Electricity\" Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7051e1d",
   "metadata": {},
   "source": [
    "- Just as electricity transformed every major industry a century ago, AI (and deep learning) is expected to bring about an equally significant transformation across all sectors of society."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccbf105",
   "metadata": {},
   "source": [
    "### Neural Networks: The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66806df2",
   "metadata": {},
   "source": [
    "- A neural network takes an input \\(x\\) and learns a function to predict an output \\(y\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e29740",
   "metadata": {},
   "source": [
    "- **Simple Example (Housing Price Prediction)**: Predicting house price (y) based on its size (x)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa37346",
   "metadata": {},
   "source": [
    "- **Single Neuron**: In its simplest form, a single neuron can implement a function like predicting house price from size, ensuring the output is non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47a0be",
   "metadata": {},
   "source": [
    "- **ReLU Function**: A common activation function used in neurons, which stands for Rectified Linear Unit. It outputs \\( \\max(0, ext{input}) \\), ensuring non-negativity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48e455",
   "metadata": {},
   "source": [
    "### Building Larger Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda054c",
   "metadata": {},
   "source": [
    "- Larger neural networks are formed by stacking many single neurons (like Lego bricks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a37e8",
   "metadata": {},
   "source": [
    "- **Input Features**: Multiple characteristics (e.g., size, number of bedrooms, zip code, wealth of neighborhood) serve as inputs to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6af67",
   "metadata": {},
   "source": [
    "- **Hidden Units**: The circles in the middle layers of the network are called hidden units. Each hidden unit can take all input features as input, allowing the network to automatically discover complex relationships (e.g., how size and bedrooms relate to family size)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd12c83",
   "metadata": {},
   "source": [
    "- **Dense Connection**: Every input feature is connected to every unit in the subsequent hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af7e80",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3136559",
   "metadata": {},
   "source": [
    "- In supervised learning, you have paired input-output data \\((x, y)\\) and aim to learn a function that maps \\(x\\) to \\(y\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dedc934",
   "metadata": {},
   "source": [
    "- **Key Applications**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1faa7",
   "metadata": {},
   "source": [
    "- **Online Advertising**: Input ad/user info \\(x\\), predict click probability \\(y\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aebfdb",
   "metadata": {},
   "source": [
    "- **Computer Vision**: Input image \\(x\\), output object label \\(y\\) (e.g., photo tagging)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892d8e9",
   "metadata": {},
   "source": [
    "- **Speech Recognition**: Input audio clip \\(x\\), output text transcript \\(y\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289182e3",
   "metadata": {},
   "source": [
    "- **Machine Translation**: Input English sentence \\(x\\), output Chinese sentence \\(y\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee27f67f",
   "metadata": {},
   "source": [
    "- **Autonomous Driving**: Input image/radar data \\(x\\), output position of other cars \\(y\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa284470",
   "metadata": {},
   "source": [
    "### Types of Neural Networks for Different Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293604c5",
   "metadata": {},
   "source": [
    "- **Standard Neural Networks**: Used for tabular data like real estate prediction or online advertising (where features are independent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433be9de",
   "metadata": {},
   "source": [
    "- **Convolutional Neural Networks (CNNs)**: Primarily used for image data, excelling at capturing spatial hierarchies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ad04c",
   "metadata": {},
   "source": [
    "- **Recurrent Neural Networks (RNNs)**: Ideal for sequence data, such as audio (time-series) and natural language (sequence of words), due to their ability to model temporal dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0667b",
   "metadata": {},
   "source": [
    "- **Hybrid Architectures**: For complex problems like autonomous driving, a combination of CNNs (for images) and other networks (for radar data) may be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110cf4f",
   "metadata": {},
   "source": [
    "### Structured vs. Unstructured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2809c0b0",
   "metadata": {},
   "source": [
    "- **Structured Data**: Data organized in databases, where each feature has a well-defined meaning (e.g., house size, number of bedrooms, user age). Historically easier for computers to process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750d600b",
   "metadata": {},
   "source": [
    "- **Unstructured Data**: Raw data like images (pixel values), audio (waveforms), or text (individual words). Deep learning has significantly improved computers' ability to interpret unstructured data, leading to many new applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3dbd2",
   "metadata": {},
   "source": [
    "### Why is Deep Learning Taking Off Now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04031a5",
   "metadata": {},
   "source": [
    "Despite underlying ideas existing for decades, deep learning's recent success is attributed to three main drivers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd861bd",
   "metadata": {},
   "source": [
    "1. **Data Scale**: The digitization of society, widespread sensors (cell phones, IoT), and increased human activity in the digital realm have led to an explosion of available labeled data (\\(m\\)). Large neural networks excel with vast amounts of data, often outperforming traditional algorithms which plateau earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99282725",
   "metadata": {},
   "source": [
    "2. **Computational Power**: Advances in hardware, especially the rise of GPUs (Graphics Processing Units), have made it feasible to train very large neural networks much faster. This allows for training bigger networks and processing more data efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1231d24",
   "metadata": {},
   "source": [
    "3. **Algorithmic Innovation**: While fundamental, continuous algorithmic improvements have significantly boosted efficiency. For example, switching from sigmoid to ReLU activation functions dramatically speeds up gradient descent by avoiding vanishing gradients, allowing faster learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef4a80",
   "metadata": {},
   "source": [
    "- **Faster Iteration Cycle**: The combination of improved computation and algorithms enables researchers and practitioners to quickly iterate on ideas (idea -> implement -> experiment -> change), accelerating discovery and improvement of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea0a87",
   "metadata": {},
   "source": [
    "### Interview with Geoffrey Hinton: The \"Godfather of Deep Learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c6425",
   "metadata": {},
   "source": [
    "- **Early Inspiration**: Hinton's interest in how the brain stores memories began in high school."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3338fa",
   "metadata": {},
   "source": [
    "- **Backpropagation**: Co-developed the backpropagation algorithm (with Rumelhart and Williams) in the early 1980s, which became a cornerstone for training neural networks. Their 1986 Nature paper was instrumental in its acceptance, demonstrating how NNs could learn meaningful representations (early word embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b6e30",
   "metadata": {},
   "source": [
    "- **Boltzmann Machines & RBMs**: His work with Terry Sejnowski on Boltzmann Machines and later Restricted Boltzmann Machines (RBMs) provided a principled way to learn hidden representations and stack layers, contributing to the resurgence of neural nets around 2007."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab0298",
   "metadata": {},
   "source": [
    "- **Capsules**: Current research interest focuses on \"Capsule Networks,\" a novel architecture aimed at improving generalization from limited data and handling viewpoint changes more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b987f",
   "metadata": {},
   "source": [
    "- **Evolution of AI Thinking**: Initially believed unsupervised learning would dominate, but acknowledges supervised learning's current success. Still believes unsupervised learning will be crucial long-term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d0c56",
   "metadata": {},
   "source": [
    "- **Advice for Breaking into Deep Learning**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f6a62a",
   "metadata": {},
   "source": [
    "- Read the literature, but don't read too much. Look for things everyone else is doing wrong and try to fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8344d4b4",
   "metadata": {},
   "source": [
    "- Trust your intuitions, especially if others disagree; it might signify a novel idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f2608",
   "metadata": {},
   "source": [
    "- Never stop programming; practical implementation reveals crucial details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f349dc",
   "metadata": {},
   "source": [
    "## Visual Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164a1c43",
   "metadata": {},
   "source": [
    "### Housing Price Prediction Curve (ReLU-like Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8955e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sizes = np.linspace(0, 2000, 100)\n",
    "# A simple linear function, then rectified (ReLU-like)\n",
    "prices = np.maximum(0, 0.2 * sizes - 50)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(sizes, prices, color='blue', linewidth=3, label='Predicted Price')\n",
    "plt.scatter([200, 500, 700, 1000, 1200, 1500], [0, 0, 90, 150, 190, 250], color='red', label='Training Data') # Example data points\n",
    "plt.title('Housing Price Prediction based on Size')\n",
    "plt.xlabel('Size (sq ft)')\n",
    "plt.ylabel('Price (thousands $)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75618b",
   "metadata": {},
   "source": [
    "### Simple Neural Network Diagram (Single Neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d07d4c",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    A[Hello] --> B[World]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ed9fb",
   "metadata": {},
   "source": [
    "```mermaid graph LR X[\"Size (x)\"] --> A{Neuron (ReLU)} --> Y[\"Price (y)\"] ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c21cf",
   "metadata": {},
   "source": [
    "### Larger Neural Network Diagram (Multi-feature, Hidden Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a35c4",
   "metadata": {},
   "source": [
    "```mermaid graph TD subgraph Input_Layer[\"\"Input Features\"] X1[\"Size\"] X2[\" Bedrooms\"\"] X3[\"Zip Code\"] X4[\"Wealth\"] end subgraph Hidden_Layer[\"Hidden Layer\"] H1(Family Size) H2(Walkability) H3(School Quality) end subgraph Output_Layer[\"Output\"] Y(Price) end X1 --> H1 X2 --> H1 X1 --> H2 X2 --> H2 X3 --> H2 X3 --> H3 X4 --> H3 H1 --> Y H2 --> Y H3 --> Y ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a543c6",
   "metadata": {},
   "source": [
    "### Performance vs. Amount of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Amount of data (m)\n",
    "md = np.logspace(1, 4, 100) # From 10 to 10,000 data points\n",
    "\n",
    "# Traditional algorithm performance\n",
    "trad_perf = 0.6 * (1 - np.exp(-md / 500)) + 0.5\n",
    "\n",
    "# Neural network performance curves\n",
    "small_nn_perf = 0.5 * (1 - np.exp(-md / 100)) + 0.5\n",
    "medium_nn_perf = 0.65 * (1 - np.exp(-md / 200)) + 0.5\n",
    "large_nn_perf = 0.8 * (1 - np.exp(-md / 300)) + 0.5\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(md, trad_perf, label='Traditional Algorithm (SVM, Logistic Regression)', linestyle='--', color='gray')\n",
    "plt.plot(md, small_nn_perf, label='Small Neural Network', color='orange')\n",
    "plt.plot(md, medium_nn_perf, label='Medium Neural Network', color='green')\n",
    "plt.plot(md, large_nn_perf, label='Large Neural Network', color='blue')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.ylim(0.4, 1.0)\n",
    "plt.title('Performance of Learning Algorithms vs. Amount of Labeled Data (m)')\n",
    "plt.xlabel('Amount of Labeled Data (m)')\n",
    "plt.ylabel('Performance (e.g., Accuracy)')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b904cf8",
   "metadata": {},
   "source": [
    "### Sigmoid vs. ReLU Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc412365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "relu = np.maximum(0, z)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(z, sigmoid, label='Sigmoid', color='purple')\n",
    "plt.title('Sigmoid Activation Function')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(z, relu, label='ReLU', color='teal')\n",
    "plt.title('ReLU (Rectified Linear Unit) Activation Function')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38072a2b",
   "metadata": {},
   "source": [
    "### Deep Learning Iteration Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795431e2",
   "metadata": {},
   "source": [
    "```mermaid graph LR A[Idea] --> B(Implement) B --> C{Experiment} C --> D[Change/Refine] D --> A ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf961b6",
   "metadata": {},
   "source": [
    "## Important Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1408a1",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4404974",
   "metadata": {},
   "source": [
    "The ReLU function is a widely used activation function in neural networks due to its computational efficiency and ability to mitigate the vanishing gradient problem. It outputs the input directly if it is positive, otherwise, it outputs zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5804a3",
   "metadata": {},
   "source": [
    "$$ ext{ReLU}(z) = \\max(0, z) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21726b2d",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ecfc7",
   "metadata": {},
   "source": [
    "The sigmoid function squashes its input to a range between 0 and 1. While historically popular, it suffers from vanishing gradients, especially for very large or very small inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6eca5",
   "metadata": {},
   "source": [
    "$$ \\sigma(z) = rac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6c9db",
   "metadata": {},
   "source": [
    "## Practical Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83098fc7",
   "metadata": {},
   "source": [
    "- **Neural Networks as Lego Bricks**: Imagine each simple neuron (like the one predicting house prices from size) as a single Lego brick. You can build much larger and more complex structures (larger neural networks) by stacking these bricks together in various configurations, allowing them to learn incredibly sophisticated patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a51a7e",
   "metadata": {},
   "source": [
    "- **Deep Learning's Pillars of Progress**: The rapid advancement of deep learning is like a sturdy tripod, supported by three crucial legs: abundant **data** (fueling the learning), powerful **computation** (to process the data and train complex models), and clever **algorithms** (making the learning process efficient and effective)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf79a15",
   "metadata": {},
   "source": [
    "- **The Experimentation Loop**: Developing a deep learning model often feels like an iterative scientific experiment. You start with an *idea* for an architecture, *implement* it in code, *experiment* by training it, analyze its performance, then *change* something based on the results, and repeat. The faster you can complete this loop, the quicker you'll find an effective solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3312e",
   "metadata": {},
   "source": [
    "## Quick Revision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f609e",
   "metadata": {},
   "source": [
    "- Deep learning uses large neural networks to solve complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5bc79",
   "metadata": {},
   "source": [
    "- It is compared to \"new electricity\" for its transformative potential across industries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baabbf2",
   "metadata": {},
   "source": [
    "- Supervised learning, mapping \\(x\\) to \\(y\\), is the most successful application area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406b292",
   "metadata": {},
   "source": [
    "- Basic neural networks consist of input, hidden, and output layers with densely connected units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab085de",
   "metadata": {},
   "source": [
    "- CNNs handle images, RNNs handle sequences; standard NNs are for structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35fbab",
   "metadata": {},
   "source": [
    "- Deep learning's rise is due to massive data, powerful computation (GPUs), and algorithmic improvements (e.g., ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9854c1",
   "metadata": {},
   "source": [
    "- Faster iteration cycles are critical for developing effective deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13cf179",
   "metadata": {},
   "source": [
    "- Geoffrey Hinton emphasized following intuition and constant programming as keys to innovation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654fe7f",
   "metadata": {},
   "source": [
    "## Practice Questions (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1c0d34",
   "metadata": {},
   "source": [
    "1. How does the \"AI is the new electricity\" analogy relate to the current impact and future potential of deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9e4f96",
   "metadata": {},
   "source": [
    "2. Describe the core components of a simple neural network using the housing price prediction example. What role does the ReLU function play?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d15d0c",
   "metadata": {},
   "source": [
    "3. Explain the difference between structured and unstructured data, and provide an example of how a different type of neural network might be used for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520514aa",
   "metadata": {},
   "source": [
    "4. What are the three main factors driving the current success of deep learning? How do they interact to accelerate progress?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c6b88",
   "metadata": {},
   "source": [
    "5. Based on Geoffrey Hinton's advice, what is a counter-intuitive approach one might take when trying to innovate in a field like deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea803004",
   "metadata": {},
   "source": [
    "# Neural Network Basics: Binary Classification and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf33045",
   "metadata": {},
   "source": [
    "# Neural Network Basics: Binary Classification and Gradient Descent *Understand the foundations of neural networks, including binary classification, logistic regression, gradient descent, and vectorization techniques.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9f7a3",
   "metadata": {},
   "source": [
    "## Key Takeaways - Neural networks involve forward and backward propagation steps. - Logistic Regression is a fundamental algorithm for binary classification. - Images are flattened into feature vectors for model input. - The sigmoid function maps any real value to a probability between 0 and 1. - The Loss Function quantifies error for a single training example, while the Cost Function averages loss over the entire training set. - Gradient Descent is an optimization algorithm used to minimize the cost function by iteratively adjusting parameters. - Derivatives represent the slope of a function, indicating the direction and magnitude of change. - Computation Graphs visualize the steps to compute a function and its derivatives. - Vectorization eliminates explicit for-loops, significantly speeding up computations in deep learning. - Broadcasting in NumPy allows operations on arrays of different shapes by automatically expanding the smaller array. - Avoid NumPy rank-1 arrays; prefer explicit column `(n,1)` or row `(1,n)` vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88203ac",
   "metadata": {},
   "source": [
    "## Concepts Explained ### Binary Classification Problem - **Definition**: A machine learning task where the output label \\(y\\) is one of two classes, typically represented as 0 or 1. - **Example**: Classifying an image as a \"cat\" (1) or \"not-cat\" (0). ### Image Representation - **Structure**: An image is represented by three matrices (Red, Green, Blue color channels). - **Feature Vector \\(x\\)**: These matrices are \"unrolled\" into a single, long column vector. For a 64x64 pixel image, \\(x\\) would be 64 * 64 * 3 = 12,288 dimensions. - **Notation**: \\(n_x\\) (or just \\(n\\)) denotes the dimension of the input feature vector \\(x\\). ### Training Set Notation - **Single Example**: A pair \\((x^{(i)}, y^{(i)})\\) where \\(x^{(i)}\\) is the \\(n_x\\)-dimensional feature vector and \\(y^{(i)}\\) is its label (0 or 1). - **Training Set Size**: Lowercase \\(m\\) denotes the number of training examples. - **Matrix \\(X\\)**: All \\(m\\) training input vectors \\(x^{(1)}, x^{(2)}, \\dots, x^{(m)}\\) are stacked horizontally as columns. Thus, \\(X\\) is an \\((n_x, m))\\) matrix. - **Matrix \\(Y\\)**: All \\(m\\) training output labels \\(y^{(1)}, y^{(2)}, \\dots, y^{(m)}\\) are stacked horizontally as columns. Thus, \\(Y\\) is a \\((1, m))\\) matrix (a row vector). ### Logistic Regression Model - **Goal**: Given input \\(x\\), predict \\(y\\) (0 or 1) by estimating the probability \\(P(y=1|x))\\), denoted as \\(\\hat{y}\\). - **Parameters**: Weights \\(w\\) (an \\(n_x\\)-dimensional vector) and bias \\(b\\) (a real number). - **Linear Combination**: \\(z = w^T x + b\\). - **Activation**: \\(\\hat{y} = \\sigma(z))\\), where \\(\\sigma\\) is the sigmoid function. ### Sigmoid Function - **Formula**: \\(\\sigma(z) = rac{1}{1 + e^{-z}}\\) . - **Output Range**: Maps any real number \\(z\\) to a value between 0 and 1. - **Interpretation**: In logistic regression, \\(\\hat{y}\\) is interpreted as a probability. - **Behavior**: - If \\(z\\) is very large, \\(\\sigma(z) \u0007pprox 1\\). - If \\(z\\) is very small (large negative), \\(\\sigma(z) \u0007pprox 0\\). - \\(\\sigma(0) = 0.5\\). ### Loss Function - **Purpose**: Measures how well the model's prediction \\(\\hat{y}\\) aligns with the true label \\(y\\) for a single training example. - **Formula (Binary Cross-Entropy Loss)**: $$L(\\hat{y}, y) = -(y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y}))$$ - **Intuition**: - If \\(y=1\\), \\(L = -\\log(\\hat{y}))\\). To minimize loss, \\(\\hat{y}\\) should be close to 1. - If \\(y=0\\), \\(L = -\\log(1-\\hat{y}))\\). To minimize loss, \\(\\hat{y}\\) should be close to 0. - **Why not squared error?**: Squared error leads to a non-convex optimization problem with multiple local optima, making gradient descent difficult. ### Cost Function - **Purpose**: Measures the overall performance of the parameters \\((w, b))\\) on the entire training set. - **Formula**: The average of the loss function over all \\(m\\) training examples. $$J(w, b) = rac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)}))$$ - **Goal**: Find \\(w, b\\) that minimize \\(J(w, b))\\). ### Gradient Descent Algorithm - **Purpose**: An iterative optimization algorithm to find the parameters \\((w, b))\\) that minimize the cost function \\(J(w, b))\\). - **Initialization**: Initialize \\(w\\) and \\(b\\) (e.g., to zeros). - **Update Rule**: Repeatedly update parameters in the direction of the steepest decrease of the cost function. - \\(w := w - \u0007lpha rac{\\partial J}{\\partial w}\\) - \\(b := b - \u0007lpha rac{\\partial J}{\\partial b}\\) where \\(\u0007lpha\\) is the learning rate (controls step size). - **Convexity**: For logistic regression, \\(J(w, b))\\) is a convex function, ensuring gradient descent converges to a global minimum. ### Derivatives - **Intuition**: The derivative of a function at a point represents its **slope** at that point. It indicates how much the function's output changes in response to a tiny change in its input. - **Notation**: \\(rac{df(a)}{da}\\) or \\(rac{\\partial J}{\\partial w}\\) (for partial derivatives when a function has multiple inputs). - **Chain Rule**: If \\(J\\) depends on \\(v\\), and \\(v\\) depends on \\(a\\), then \\(rac{dJ}{da} = rac{dJ}{dv} \\cdot rac{dv}{da}\\). This is fundamental for backpropagation. - **Coding Convention**: In code, `dw` often denotes \\(rac{\\partial J}{\\partial w}\\) and `db` denotes \\(rac{\\partial J}{\\partial b}\\). ### Computation Graph - **Concept**: A visual representation of a function's computation steps, showing inputs, intermediate variables, and outputs as nodes, and operations as edges. - **Forward Pass (Left-to-Right)**: Computes the output of the function (e.g., the cost \\(J\\)). - **Backward Pass (Right-to-Left / Backpropagation)**: Computes the derivatives of the output with respect to inputs and intermediate variables, efficiently using the chain rule. ### Backpropagation for Logistic Regression (Single Example) - **Inputs**: \\(x_1, x_2, \\dots, x_{n_x}\\), \\(w_1, w_2, \\dots, w_{n_x}\\), \\(b\\). - **Forward Steps**: 1. \\(z = w_1 x_1 + w_2 x_2 + \\dots + w_{n_x} x_{n_x} + b\\) 2. \\(a = \\sigma(z)\\) (\\(\\hat{y}\\)) 3. \\(L = -(y \\log(a) + (1-y) \\log(1-a))\\) - **Backward Steps (Derivatives)**: 1. \\(rac{\\partial L}{\\partial a} = -rac{y}{a} + rac{1-y}{1-a}\\) 2. \\(rac{\\partial L}{\\partial z} = a - y\\) (combines \\(rac{\\partial L}{\\partial a}\\) and \\(rac{\\partial a}{\\partial z}\\)) 3. \\(rac{\\partial L}{\\partial w_1} = x_1 \\cdot rac{\\partial L}{\\partial z}\\) 4. \\(rac{\\partial L}{\\partial w_2} = x_2 \\cdot rac{\\partial L}{\\partial z}\\) (and similarly for other \\(w_j\\)) 5. \\(rac{\\partial L}{\\partial b} = rac{\\partial L}{\\partial z}\\) ### Gradient Descent on \\(m\\) Examples (Non-Vectorized) - **Algorithm**: 1. Initialize \\(J=0, dw_1=0, \\dots, dw_{n_x}=0, db=0\\). 2. **For** \\(i = 1\\) to \\(m\\): a. Compute \\(z^{(i)} = w^T x^{(i)} + b\\). b. Compute \\(a^{(i)} = \\sigma(z^{(i)})\\). c. Accumulate cost: \\(J += L(a^{(i)}, y^{(i)})\\). d. Compute derivatives for current example: \\(dz^{(i)} = a^{(i)} - y^{(i)}\\). e. Accumulate gradients: \\(dw_j += x_j^{(i)} dz^{(i)}\\) for all \\(j\\), and \\(db += dz^{(i)}\\). 3. Average cost and gradients: \\(J /= m\\), \\(dw_j /= m\\), \\(db /= m\\). 4. Update parameters: \\(w_j := w_j - \u0007lpha dw_j\\), \\(b := b - \u0007lpha db\\). - **Weakness**: Involves two explicit for-loops (one over \\(m\\) examples, one over \\(n_x\\) features), which is computationally inefficient. ### Vectorization - **Concept**: Replacing explicit for-loops with highly optimized array operations (e.g., NumPy functions). - **Benefits**: - **Speed**: Achieves significant speedup (e.g., 100x-300x) by leveraging parallelization capabilities of CPUs/GPUs (SIMD instructions). - **Clarity**: Often results in more concise and readable code. - **Rule of Thumb**: Whenever possible, avoid explicit for-loops in deep learning implementations. ### Vectorizing Logistic Regression (Forward Pass) - **Goal**: Compute all \\(z^{(i)}\\) and \\(a^{(i)}\\) for all \\(m\\) training examples simultaneously. - **Matrix \\(X\\)**: \\((n_x, m))\\) matrix of input features. - **Weights \\(W\\)**: \\((n_x, 1))\\) column vector. - **Bias \\(b\\)**: A scalar. - **Computation**: 1. \\(Z = W^T X + b\\) (where \\(b\\) is broadcasted to a \\((1,m))\\) row vector). \\(Z\\) is a \\((1,m))\\) matrix. 2. \\(A = \\sigma(Z)\\) (element-wise sigmoid function). \\(A\\) is a \\((1,m))\\) matrix. - **NumPy**: `Z = np.dot(W.T, X) + b`, `A = sigmoid(Z)`. ### Vectorizing Logistic Regression (Backward Pass) - **Goal**: Compute all derivatives \\(rac{\\partial J}{\\partial W}\\) and \\(rac{\\partial J}{\\partial b}\\) for all \\(m\\) examples simultaneously. - **Derivatives**: 1. \\(dZ = A - Y\\). \\(dZ\\) is a \\((1,m))\\) matrix. 2. \\(dW = rac{1}{m} X dZ^T\\). \\(dW\\) is an \\((n_x, 1))\\) matrix. 3. \\(db = rac{1}{m} \\sum_{i=1}^{m} dz^{(i)}\\). \\(db\\) is a scalar. - **NumPy**: `dZ = A - Y`, `dW = (1/m) * np.dot(X, dZ.T)`, `db = (1/m) * np.sum(dZ)`. - **Overall Gradient Descent Iteration**: 1. Compute \\(Z\\) and \\(A\\) (forward pass). 2. Compute \\(dZ\\), \\(dW\\), \\(db\\) (backward pass). 3. Update \\(W := W - \u0007lpha dW\\), \\(b := b - \u0007lpha db\\). This single iteration is performed without any explicit for-loops over examples or features. ### Broadcasting in Python (NumPy) - **Concept**: Enables arithmetic operations between arrays of different shapes, often by \"stretching\" the smaller array to match the larger one. - **Examples**: - Scalar + vector: `[1,2,3] + 100` -> `[101,102,103]` - (m,n) matrix + (1,n) row vector: The row vector is copied \\(m\\) times vertically. - (m,n) matrix + (m,1) column vector: The column vector is copied \\(n\\) times horizontally. - **Utility**: Reduces code complexity and can implicitly vectorize operations. - **Tip**: Use `.reshape()` to explicitly ensure arrays have the desired shape, especially when broadcasting, to avoid subtle bugs. ### Python-NumPy Vector Best Practices - **Avoid Rank-1 Arrays**: `np.random.randn(5)` creates a rank-1 array of shape `(5,)`, which behaves inconsistently (e.g., `a.T` is `a`, `np.dot(a, a.T)` is a scalar). - **Explicit Dimensions**: Always define vectors as either column vectors `(n,1)` or row vectors `(1,n)` using `np.random.randn(n, 1)` or `np.random.randn(1, n)`. - **Assertions**: Use `assert(a.shape == (n,1))` to debug and document expected dimensions. - **Reshape**: Use `a.reshape(n, 1)` or `a.reshape(1, n)` to explicitly set vector dimensions if you encounter a rank-1 array. - **Benefit**: Simplifies code logic and eliminates hard-to-find bugs related to inconsistent vector behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ef068",
   "metadata": {},
   "source": [
    "## Visual Understanding ### Computation Graph Example ```mermaid graph LR A[\"a] --> V_node(v = a + u) B[b] --> U_node(u = b * c) C[c] --> U_node U_node --> V_node V_node --> J_node(J = 3 * v) ``` ## Broadcasting Example ```mermaid graph TD subgraph Step_1_Matrix_A[\"Matrix A (3x4)\"\"] A_row1[\"[\"56, 104, 1.2, 12\"]\"] A_row2[\"[\"1.2, 13, 93, 2\"]\"] A_row3[\"[\"1.8, 135, 98, 5\"]\"] end subgraph Step_2_Sum_Columns[\"Sum Columns (axis=0)\"] Cal[\"[\"59, 239, 192.2, 19\"]\"] end subgraph Step_3_Reshape_Cal[\"Reshape Cal (1x4)\"] Reshaped_Cal[\"[\"[59, 239, 192.2, 19\"]]\"] end subgraph Step_4_Broadcasting_Division[\"Broadcasting Division: A / Reshaped_Cal\"] A_div_Cal[\"Resulting (3x4) Percentage Matrix\"] end A_row1 --- Cal A_row2 --- Cal A_row3 --- Cal Cal --> Reshaped_Cal Reshaped_Cal --- A_div_Cal style Cal fill:#f9f,stroke:#333,stroke-width:2px style Reshaped_Cal fill:#9ff,stroke:#333,stroke-width:2px ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1fe39",
   "metadata": {},
   "source": [
    "## Important Formulas - **Logistic Regression Linear Combination**: $$z = w^T x + b$$ - **Sigmoid Activation**: $$\\hat{y} = a = \\sigma(z) = rac{1}{1 + e^{-z}}$$ - **Loss Function (Binary Cross-Entropy)**: $$L(\\hat{y}, y) = -(y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y}))$$ - **Cost Function**: $$J(w, b) = rac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)}))$$ - **Gradient Descent Update Rules (Scalar)**: $$w := w - \u0007lpha rac{\\partial J}{\\partial w}$$ $$b := b - \u0007lpha rac{\\partial J}{\\partial b}$$ - **Derivatives for Logistic Regression (Single Example)**: $$dz = a - y$$ $$dw_j = x_j \\cdot dz$$ $$db = dz$$ - **Vectorized Forward Pass**: $$Z = W^T X + b$$ $$A = \\sigma(Z)$$ - **Vectorized Backward Pass**: $$dZ = A - Y$$ $$dW = rac{1}{m} X dZ^T$$ $$db = rac{1}{m} ext{np.sum}(dZ)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc86b9",
   "metadata": {},
   "source": [
    "## Practical Understanding - **Analogy (Gradient Descent)**: Imagine you're blindfolded on a mountain and want to find the lowest point. You can only feel the slope directly under your feet. Gradient descent is like taking a small step in the direction that feels steepest downhill. The learning rate \\(\u0007lpha\\) is how big a step you take. - **Analogy (Vectorization)**: Think of a supermarket checkout. A non-vectorized approach is like each customer having to process their items one by one. A vectorized approach is like a modern scanner that can process an entire cart of items simultaneously (or many items in parallel), making the checkout much faster. - **Analogy (Broadcasting)**: Imagine you have a list of prices for different items and you want to add tax to each. Broadcasting is like having the tax rate (a single number) automatically applied to every item on your list without you having to manually write a loop for each item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65552f3e",
   "metadata": {},
   "source": [
    "## Quick Revision - Binary classification outputs 0 or 1. - Images are flattened into a single feature vector \\(x\\). - \\(X\\) and \\(Y\\) matrices stack training examples as columns. - Logistic regression uses sigmoid to output probabilities. - Loss is for a single example, Cost is for the entire training set. - Gradient descent minimizes the cost function by adjusting \\(w\\) and \\(b\\). - Derivatives are slopes; chain rule is key for backprop. - Computation graphs visualize calculations, enabling forward and backward passes. - Vectorization replaces loops for huge speedups. - Broadcasting simplifies operations on arrays of different shapes. - Always use explicit `(n,1)` or `(1,n)` NumPy vectors, avoid rank-1 arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a8560",
   "metadata": {},
   "source": [
    "## Practice Questions (Optional) 1. Explain why the sigmoid function is preferred over a simple linear function (\\(w^T x + b\\)) for binary classification. 2. What is the main advantage of using the binary cross-entropy loss function over the squared error loss for logistic regression? 3. Describe the key difference between the loss function and the cost function in the context of machine learning. 4. Why is vectorization considered a \"key skill\" in the deep learning era, and how does it achieve speedup? 5. What is a NumPy \"rank-1 array,\" and why is it recommended to avoid using them in deep learning implementations? 6. Briefly explain how broadcasting works in NumPy when you add a `(m,n)` matrix to a `(1,n)` row vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c163cd3",
   "metadata": {},
   "source": [
    "# Shallow Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2879b361",
   "metadata": {},
   "source": [
    "# Shallow Neural Networks *Understand the architecture, forward propagation, and backpropagation for neural networks with a single hidden layer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f290b",
   "metadata": {},
   "source": [
    "## Key Takeaways - Neural networks can be thought of as stacking multiple logistic regression units. - A two-layer neural network includes an input layer (layer 0), a hidden layer (layer 1), and an output layer (layer 2). - Non-linear activation functions are crucial in hidden layers to enable the network to learn complex non-linear relationships. - Initializing weights to zero leads to a \"symmetry breaking\" problem, making hidden units compute identical functions. - Random initialization (with small values) for weights is essential to allow hidden units to learn diverse features. - Forward propagation computes predictions, while backpropagation computes gradients needed for parameter updates. - Vectorization across multiple training examples significantly speeds up computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f86c95c",
   "metadata": {},
   "source": [
    "## Concepts Explained ### Neural Network Overview - A neural network extends logistic regression by stacking multiple computational units (neurons). - Each unit performs two steps: a linear transformation (computing `z`) followed by a non-linear activation (computing `a`). - The network processes input `x` through a series of layers to produce an output `y_hat`. ### Neural Network Representation - **Input Layer (Layer 0)**: Contains the input features, denoted as \\( A^{[0]} = X \\). - **Hidden Layer (Layer 1)**: Processes the input and generates intermediate activations, \\( A^{[1]} \\). These values are not directly observed in the training data. - **Output Layer (Layer 2)**: Produces the final prediction, \\( A^{[2]} = \\hat{y} \\). - **Layer Counting Convention**: The input layer is typically not counted. A network with one hidden layer is referred to as a \"two-layer neural network.\" - **Notation**: - Superscript `[l]` refers to quantities associated with layer \\( l \\) (e.g., \\( W^{[1]} \\), \\( b^{[1]} \\), \\( Z^{[1]} \\), \\( A^{[1]} \\)). - Superscript `(i)` refers to the \\( i^{th} \\) training example (e.g., \\( x^{(i)} \\)). - Subscript `_j` refers to the \\( j^{th} \\) node in a layer. ### Computing a Neural Network's Output (Forward Propagation) - Each node in a layer performs two steps: 1. Compute a weighted sum of inputs plus a bias: \\( z = w^T x + b \\). 2. Apply an activation function: \\( a = g(z) \\). - For a two-layer network, this involves: - Layer 1 (Hidden Layer): \\( Z^{[1]} = W^{[1]}X + b^{[1]} \\) and \\( A^{[1]} = g^{[1]}(Z^{[1]}) \\) - Layer 2 (Output Layer): \\( Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} \\) and \\( A^{[2]} = g^{[2]}(Z^{[2]}) \\) - \\( A^{[2]} \\) represents the final prediction \\( \\hat{y} \\). ### Vectorizing Across Multiple Examples - To efficiently compute predictions for `m` training examples, we stack them horizontally into matrices. - \\( X \\) becomes an \\( (n_0, m) \\) matrix (\\( n_0 \\) features, \\( m \\) examples). - The computations become matrix operations: - \\( Z^{[1]} = W^{[1]}X + b^{[1]} \\) - \\( A^{[1]} = g^{[1]}(Z^{[1]}) \\) - \\( Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} \\) - \\( A^{[2]} = g^{[2]}(Z^{[2]}) \\) - Here, \\( W^{[l]} \\) is \\( (n_l, n_{l-1}) \\), \\( b^{[l]} \\) is \\( (n_l, 1) \\), and \\( Z^{[l]} \\) and \\( A^{[l]} \\) are \\( (n_l, m) \\) matrices. - Python's broadcasting handles the addition of \\( b^{[l]} \\) to each column of \\( W^{[l]}A^{[l-1]} \\). ### Why Non-Linear Activation Functions? - Using only linear activation functions (\\( g(z) = z \\)) in hidden layers would cause the entire neural network to collapse into a single linear model. - The composition of two or more linear functions is still a linear function (e.g., \\( A^{[2]} = W^{[2]}(W^{[1]}X + b^{[1]}) + b^{[2]} = (W^{[2]}W^{[1]})X + (W^{[2]}b^{[1]} + b^{[2]}) \\), which is effectively \\( W'X + b' \\)). - Non-linearity allows neural networks to learn complex, non-linear decision boundaries and representations. - The only common exception for a linear activation function is in the output layer for regression problems where the output \\( \\hat{y} \\) is a real number (e.g., predicting housing prices). ### Gradient Descent for Neural Networks - **Parameters**: \\( W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]} \\). - \\( W^{[1]} \\) has dimensions \\( (n_1, n_0) \\), \\( b^{[1]} \\) has \\( (n_1, 1) \\). - \\( W^{[2]} \\) has dimensions \\( (n_2, n_1) \\), \\( b^{[2]} \\) has \\( (n_2, 1) \\). - **Cost Function**: For binary classification, \\( J(W,b) = -rac{1}{m} \\sum_{i=1}^{m} [ y^{(i)}\\log a^{[2](i)} + (1-y^{(i)})\\log(1-a^{[2](i)}) ] \\). - **Gradient Descent Steps**: 1. **Initialize parameters randomly** (important for \\( W \\), not zeros). \\( b \\) can be zeros. 2. **Forward Propagation**: Compute \\( A^{[2]} \\) (predictions) for all \\( m \\) examples. 3. **Backpropagation**: Compute the gradients \\( dW^{[1]}, db^{[1]}, dW^{[2]}, db^{[2]} \\). 4. **Update Parameters**: \\( W^{[l]} = W^{[l]} - \u0007lpha \\cdot dW^{[l]} \\) and \\( b^{[l]} = b^{[l]} - \u0007lpha \\cdot db^{[l]} \\), where \\( \u0007lpha \\) is the learning rate. ### Random Initialization - **Symmetry Breaking Problem**: If all weights \\( W \\) are initialized to zero, all hidden units in a layer will compute the exact same function. This means they will all have identical gradients, and thus, update identically, making them redundant (as if there's only one hidden unit). - **Solution**: Initialize weights \\( W \\) to small random values (e.g., using `np.random.randn(shape) * 0.01`). - **Bias Initialization**: Biases \\( b \\) can be initialized to zeros without causing the symmetry problem. - **Why small random values?**: Large initial weights can lead to very large values for \\( Z \\). If \\( Z \\) is very large (positive or negative), sigmoid or tanh activation functions can \"saturate\" (output values close to 0 or 1), where their gradients are extremely small. This slows down gradient descent, making learning very slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14329234",
   "metadata": {},
   "source": [
    "## Visual Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ebb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def draw_neural_network(ax, num_input, num_hidden, num_output, layer_labels):\n",
    "    G = nx.DiGraph()\n",
    "    pos = {}\n",
    "\n",
    "    # Input layer\n",
    "    for i in range(num_input):\n",
    "        G.add_node(f\"I{i}\", layer=0)\n",
    "        pos[f\"I{i}\"] = (0, i - (num_input - 1) / 2)\n",
    "\n",
    "    # Hidden layer\n",
    "    for i in range(num_hidden):\n",
    "        G.add_node(f\"H{i}\", layer=1)\n",
    "        pos[f\"H{i}\"] = (1, i - (num_hidden - 1) / 2)\n",
    "\n",
    "    # Output layer\n",
    "    for i in range(num_output):\n",
    "        G.add_node(f\"O{i}\", layer=2)\n",
    "        pos[f\"O{i}\"] = (2, i - (num_output - 1) / 2)\n",
    "\n",
    "    # Edges (connections)\n",
    "    for i in range(num_input):\n",
    "        for j in range(num_hidden):\n",
    "            G.add_edge(f\"I{i}\", f\"H{j}\")\n",
    "\n",
    "    for i in range(num_hidden):\n",
    "        for j in range(num_output):\n",
    "            G.add_edge(f\"H{i}\", f\"O{j}\")\n",
    "\n",
    "    # Draw nodes and edges\n",
    "    node_colors = ['lightblue' if 'I' in node else 'lightgreen' if 'H' in node else 'salmon' for node in G.nodes()]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=1000, ax=ax)\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax)\n",
    "\n",
    "    # Add labels for nodes\n",
    "    node_labels = {}\n",
    "    for i in range(num_input):\n",
    "        node_labels[f\"I{i}\"] = f\"x_{i+1}\"\n",
    "    for i in range(num_hidden):\n",
    "        node_labels[f\"H{i}\"] = f\"a_{i+1}^{[1]}\"\n",
    "    for i in range(num_output):\n",
    "        node_labels[f\"O{i}\"] = f\"y_hat\"\n",
    "        if num_output > 1: # Adjust for multiple outputs if needed\n",
    "            node_labels[f\"O{i}\"] = f\"a_{i+1}^{[2]}\"\n",
    "    nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=8, ax=ax)\n",
    "\n",
    "    # Add layer labels\n",
    "    ax.text(pos['I0'][0], pos['I0'][1] + (num_input-1)/2 + 0.5, layer_labels[0], \n",
    "            horizontalalignment='center', fontsize=12, weight='bold')\n",
    "    ax.text(pos['H0'][0], pos['H0'][1] + (num_hidden-1)/2 + 0.5, layer_labels[1], \n",
    "            horizontalalignment='center', fontsize=12, weight='bold')\n",
    "    ax.text(pos['O0'][0], pos['O0'][1] + (num_output-1)/2 + 0.5, layer_labels[2], \n",
    "            horizontalalignment='center', fontsize=12, weight='bold')\n",
    "\n",
    "    ax.set_title(\"Shallow Neural Network Architecture\", fontsize=14, weight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "draw_neural_network(ax, 3, 4, 1, \n",
    "                    layer_labels=[\"Input Layer ($A^{[0]}$)\", \"Hidden Layer ($A^{[1]}$)\", \"Output Layer ($A^{[2]}$)\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a0037",
   "metadata": {},
   "source": [
    "## Important Formulas Here are the vectorized equations for forward and backward propagation for a neural network with a single hidden layer and \\(m\\) training examples. ### Forward Propagation $$ Z^{[1]} = W^{[1]}X + b^{[1]} $$ $$ A^{[1]} = g^{[1]}(Z^{[1]}) $$ $$ Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} $$ $$ A^{[2]} = g^{[2]}(Z^{[2]}) \\quad ( ext{where } A^{[2]} = \\hat{Y}) $$ ### Backward Propagation (for binary classification with sigmoid output) $$ dZ^{[2]} = A^{[2]} - Y $$ $$ dW^{[2]} = rac{1}{m} dZ^{[2]} (A^{[1]})^T $$ $$ db^{[2]} = rac{1}{m} ext{np.sum}(dZ^{[2]}, ext{axis}=1, ext{keepdims}= ext{True}) $$ $$ dZ^{[1]} = (W^{[2]})^T dZ^{[2]} ext{ * } g^{[1]'}(Z^{[1]}) $$ $$ dW^{[1]} = rac{1}{m} dZ^{[1]} X^T $$ $$ db^{[1]} = rac{1}{m} ext{np.sum}(dZ^{[1]}, ext{axis}=1, ext{keepdims}= ext{True}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ee442",
   "metadata": {},
   "source": [
    "## Practical Understanding Think of a shallow neural network like a small team trying to solve a complex problem: 1. **Input Layer (You giving information)**: You provide raw data (e.g., features of a house for sale: size, location, number of bedrooms). 2. **Hidden Layer (The specialized analysts)**: A group of experts (neurons) independently analyze the raw data. Each expert might focus on different aspects, combining them in unique non-linear ways. For example, one expert might assess \"overall desirability\" by combining size and location, another might focus on \"family-friendliness\" based on bedrooms and school district. Their internal calculations (\\( Z^{[1]} \\)) and interpretations (\\( A^{[1]} \\)) are not directly shown to you, hence \"hidden.\" 3. **Output Layer (The decision maker)**: A senior expert takes all the interpretations from the hidden layer analysts and makes a final decision or prediction (e.g., the house's predicted price, or whether it will sell quickly). **Backpropagation** is like the feedback loop: - After the decision maker (output layer) makes a prediction, you compare it to the actual outcome (e.g., the actual selling price). - If there's an error, you calculate how much each expert (hidden unit) contributed to that error, and how each initial analysis step (weight \\( W \\) and bias \\( b \\)) needs to be adjusted. This error information is passed backward through the network, allowing all the experts to refine their strategies so they perform better next time. - Non-linear activations are crucial because if the experts only processed information linearly, no matter how many you have, they'd essentially all be doing the same simple task. Non-linearity lets them learn truly diverse and complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9633dff",
   "metadata": {},
   "source": [
    "## Quick Revision - Neural networks stack linear + non-linear units. - A two-layer network has 1 hidden layer. - \\( A^{[0]} \\) is input \\( X \\), \\( A^{[1]} \\) is hidden output, \\( A^{[2]} \\) is \\( \\hat{Y} \\). - Vectorization across examples uses matrix operations for speed. - Linear activations in hidden layers make the network equivalent to a single linear model. - Random initialization of weights is critical to break symmetry among hidden units. - Small random weights prevent activation function saturation and slow learning. - Forward prop calculates predictions; Backprop computes gradients for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e58af",
   "metadata": {},
   "source": [
    "## Practice Questions (Optional) 1. Explain the difference between \\( A^{(i)} \\) and \\( A^{[l]} \\) in neural network notation. 2. Why is a neural network with a hidden layer using only linear activation functions considered no more powerful than logistic regression? 3. Describe the \"symmetry breaking\" problem and how random initialization of weights solves it. 4. In a two-layer neural network with 5 input features, 10 hidden units, and 1 output unit, what are the dimensions of \\( W^{[1]}, b^{[1]}, W^{[2]}, \\) and \\( b^{[2]} \\)? 5. When is it acceptable to use a linear activation function in a neural network, and which layer would it typically be applied to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66fc655",
   "metadata": {},
   "source": [
    "# Deep L-Layer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb73680",
   "metadata": {},
   "source": [
    "# Deep L-Layer Neural Network *Understand the architecture, notation, forward/backward propagation, and hyperparameter tuning for deep neural networks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac99b2c",
   "metadata": {},
   "source": [
    "## Key Takeaways - Deep neural networks consist of multiple hidden layers, allowing them to learn complex functions. - Layer counting excludes the input layer; a neural network with one hidden layer is a 2-layer network. - Consistent notation (e.g., \\( L \\) for layers, \\( n^{[l]} \\) for units, \\( W^{[l]} \\) for weights) is crucial. - Forward propagation involves computing \\( z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]} \\) and \\( a^{[l]} = g^{[l]}(z^{[l]}) \\) for each layer. - Matrix dimensions are critical for correct implementation, especially \\( W^{[l]} \\) as \\( (n^{[l]}, n^{[l-1]}) \\) and vectorized activations \\( A^{[l]} \\) as \\( (n^{[l]}, m) \\). - Deep networks learn hierarchical representations, composing simple features (edges) into complex ones (faces). - Backpropagation equations are derived from calculus and allow for efficient gradient computation. - Parameters (W, b) are learned, while hyperparameters (learning rate, number of layers) are set before training and tuned empirically. - The analogy between deep learning and the human brain is loose and becoming less relevant as the field advances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dcbecd",
   "metadata": {},
   "source": [
    "## Concepts Explained ### What is a Deep Neural Network? - A deep neural network is characterized by having multiple hidden layers between the input and output layers. - **Shallow vs. Deep**: Logistic regression is considered a 'shallow' 1-layer model. A network with 1 hidden layer is a '2-layer' network, still relatively shallow compared to deep networks with many hidden layers. - **Layer Counting**: We count hidden layers plus the output layer. The input layer is typically not counted. - **Intuition**: Deep networks can learn more complex and abstract representations of data by processing information through multiple levels of abstraction. They excel at learning functions that shallower models cannot, or would require exponentially more hidden units to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a364e",
   "metadata": {},
   "source": [
    "### Notation for Deep Networks - **Total Layers (L)**: \\( L \\) denotes the total number of layers in the network, excluding the input layer. For example, a network with 3 hidden layers and 1 output layer has \\( L=4 \\). - **Number of Units per Layer (n^[l])**: \\( n^{[l]} \\) is the number of neurons/units in layer \\( l \\). - \\( n^{[0]} \\) (or \\( n_x \\)) is the number of input features. - \\( n^{[L]} \\) is the number of units in the output layer. - **Activations (a^[l])**: \\( a^{[l]} \\) represents the activation values of layer \\( l \\). - \\( a^{[0]} = x \\) (input features). - \\( a^{[L]} = \\hat{y} \\) (predicted output). - **Linear Combination (z^[l])**: \\( z^{[l]} \\) is the linear combination of inputs for layer \\( l \\) before applying the activation function. - **Weights (W^[l])**: \\( W^{[l]} \\) are the weight matrices for computing \\( z^{[l]} \\) in layer \\( l \\). - **Biases (b^[l])**: \\( b^{[l]} \\) are the bias vectors for computing \\( z^{[l]} \\) in layer \\( l \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f9a844",
   "metadata": {},
   "source": [
    "### Forward Propagation in a Deep Network Forward propagation computes the output \\( \\hat{y} \\) from the input \\( x \\) by iteratively calculating \\( z^{[l]} \\) and \\( a^{[l]} \\) for each layer. - **For a single training example (non-vectorized)**: For each layer \\( l = 1, \\dots, L \\): - $$ z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]} $$ - $$ a^{[l]} = g^{[l]}(z^{[l]}) $$ Where \\( g^{[l]} \\) is the activation function for layer \\( l \\). \\( a^{[0]} \\) is initialized as the input \\( x \\). - **For the entire training set (vectorized)**: For each layer \\( l = 1, \\dots, L \\): - $$ Z^{[l]} = W^{[l]}A^{[l-1]} + B^{[l]} $$ - $$ A^{[l]} = g^{[l]}(Z^{[l]}) $$ Where \\( A^{[0]} \\) is initialized as the input matrix \\( X \\), with each column representing a training example. Python's broadcasting handles the addition of \\( B^{[l]} \\), which is effectively copied \\( m \\) times across columns. This process requires a `for` loop over layers \\( l = 1, \\dots, L \\), which is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038ad70",
   "metadata": {},
   "source": [
    "### Getting Your Matrix Dimensions Right Correct matrix dimensions are crucial for bug-free implementation. - **Parameters (W^[l], b^[l])**: - For weights \\( W^{[l]} \\): dimension is \\( (n^{[l]}, n^{[l-1]}) \\). - For biases \\( b^{[l]} \\): dimension is \\( (n^{[l]}, 1) \\). *Note: \\( dW^{[l]} \\) and \\( db^{[l]} \\) will have the same dimensions as \\( W^{[l]} \\) and \\( b^{[l]} \\), respectively.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95d67a",
   "metadata": {},
   "source": [
    "### Activation and Intermediate Values (a^[l], z^[l]) - **Single example (non-vectorized)**: - \\( a^{[l]} \\) and \\( z^{[l]} \\) dimensions: \\( (n^{[l]}, 1) \\). - **Vectorized implementation (m training examples)**: - \\( A^{[l]} \\) and \\( Z^{[l]} \\) dimensions: \\( (n^{[l]}, m) \\). - Specifically, \\( A^{[0]} \\) (input \\( X \\)) dimension: \\( (n^{[0]}, m) \\). *Note: \\( dA^{[l]} \\) and \\( dZ^{[l]} \\) will have the same dimensions as \\( A^{[l]} \\) and \\( Z^{[l]} \\), respectively.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2223bb50",
   "metadata": {},
   "source": [
    "### Why Deep Representations? Deep networks leverage hierarchical representations, learning simple features in early layers and composing them into more complex ones in deeper layers. - **Example: Face Recognition** 1. **Layer 1**: Detects simple features like edges and basic textures. 2. **Layer 2**: Combines edges to detect parts of faces (e.g., eyes, nose, mouth). 3. **Layer 3**: Assembles facial parts to recognize different faces. - **Example: Speech Recognition** 1. **Layer 1**: Detects low-level audio features (e.g., tones, pitches). 2. **Layer 2**: Combines audio features to detect basic units of sound (phonemes). 3. **Layer 3**: Assembles phonemes to recognize words. 4. **Layer 4**: Combines words to recognize phrases or sentences. - **Circuit Theory Intuition**: Certain mathematical functions (e.g., XOR of many inputs) are exponentially easier to compute with deep networks (logarithmic depth) compared to shallow networks (exponentially large hidden layer). This suggests that depth provides a more efficient way to represent certain functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b8cec",
   "metadata": {},
   "source": [
    "### Building Blocks of Deep Neural Networks Deep networks are built by chaining `forward` and `backward` functions for each layer. - **Forward Function for Layer \\( l \\)**: - **Inputs**: \\( a^{[l-1]} \\) (activations from previous layer), \\( W^{[l]} \\), \\( b^{[l]} \\). - **Outputs**: \\( a^{[l]} \\) (activations for current layer), `cache` (stores \\( z^{[l]} \\), and optionally \\( W^{[l]} \\), \\( b^{[l]} \\) for backprop). - **Computations**: \\( z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]} \\) and \\( a^{[l]} = g^{[l]}(z^{[l]}) \\). - **Backward Function for Layer \\( l \\)**: - **Inputs**: \\( da^{[l]} \\) (gradients of cost wrt activations of current layer), `cache` (contains \\( z^{[l]} \\) and optionally \\( W^{[l]} \\), \\( b^{[l]} \\)). - **Outputs**: \\( da^{[l-1]} \\) (gradients wrt previous layer's activations), \\( dW^{[l]} \\), \\( db^{[l]} \\) (gradients wrt current layer's parameters). - **Overall Process**: Start with \\( A^{[0]} = X \\), run `L` forward steps to get \\( A^{[L]} = \\hat{Y} \\) and caches. Then, initialize backprop with \\( dA^{[L]} \\) and run \\( L \\) backward steps to compute all \\( dW^{[l]} \\) and \\( db^{[l]} \\) for parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b0c42",
   "metadata": {},
   "source": [
    "### Implementing Forward and Backward Propagation Here are the specific equations used within the `forward` and `backward` functions for a vectorized implementation. #### Forward Propagation (Layer \\( l \\)) $$ Z^{[l]} = W^{[l]}A^{[l-1]} + B^{[l]} $$ $$ A^{[l]} = g^{[l]}(Z^{[l]}) $$ #### Backward Propagation (Layer \\( l \\)) 1. **Compute \\( dZ^{[l]} \\)**: $$ dZ^{[l]} = dA^{[l]} \\odot g^{'[l]}(Z^{[l]}) $$ (where \\( \\odot \\) denotes element-wise product, and \\( g^{'[l]} \\) is the derivative of the activation function.) 2. **Compute \\( dW^{[l]} \\)**: $$ dW^{[l]} = rac{1}{m} dZ^{[l]} (A^{[l-1]})^T $$ 3. **Compute \\( db^{[l]} \\)**: $$ db^{[l]} = rac{1}{m} ext{np.sum}(dZ^{[l]}, ext{axis}=1, ext{keepdims}= ext{True}) $$ 4. **Compute \\( dA^{[l-1]} \\)**: $$ dA^{[l-1]} = (W^{[l]})^T dZ^{[l]} $$ #### Initializing Backpropagation for the Output Layer (Layer \\( L \\)) For binary classification with logistic regression loss: $$ dA^{[L]} = -rac{Y}{A^{[L]}} + rac{1-Y}{1-A^{[L]}} $$ This \\( dA^{[L]} \\) is then fed into the backward function of layer \\( L \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f42",
   "metadata": {},
   "source": [
    "### Parameters vs. Hyperparameters Distinguishing between parameters and hyperparameters is crucial for understanding model training. - **Parameters**: These are the values learned by the model during training. - **Examples**: Weight matrices (\\( W^{[l]} \\)) and bias vectors (\\( b^{[l]} \\)). - **Hyperparameters**: These are values that control the learning process itself and are set *before* training. They determine the final values of the parameters. - **Examples**: - Learning rate (\\( \u0007lpha \\)) - Number of iterations (epochs) - Number of hidden layers (\\( L \\)) - Number of hidden units per layer (\\( n^{[l]} \\)) - Choice of activation functions (ReLU, Sigmoid, Tanh) - Momentum term (covered in later courses) - Mini-batch size (covered in later courses) - Regularization parameters (covered in later courses) - **Empirical Process**: Applying deep learning is highly empirical. Tuning hyperparameters often involves trying many different values, running experiments, and observing the results (e.g., cost function behavior) to find the best configuration. Intuitions for hyperparameters can vary across applications and even change over time for the same application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c76ba",
   "metadata": {},
   "source": [
    "### What Does This Have to Do with the Brain? The analogy between deep learning and the human brain is largely an oversimplification. - **Loose Analogy**: A single artificial neuron (like a logistic regression unit) can be loosely compared to a biological neuron (receiving signals, performing thresholding, sending pulses). - **Complexity**: However, a biological neuron is far more complex than a simple logistic unit. The exact learning mechanisms in the human brain (e.g., if it uses backpropagation) are still a mystery to neuroscientists. - **Functionality**: Deep learning is best understood as a powerful tool for learning flexible, complex functions to map inputs to outputs in supervised learning. - **Relevance**: While historically useful for popular imagination, the brain analogy is becoming less relevant as the field matures and focuses on the mathematical and engineering aspects of deep learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf9f8d",
   "metadata": {},
   "source": [
    "## Visual Understanding ### Forward and Backward Propagation Flow Here is a simplified flow of how forward and backward propagation work through a deep neural network, highlighting the role of the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"```mermaid\n",
    "flowchart LR\n",
    "    subgraph Input_Layer[Input]\n",
    "        A0[X = a^[0]]\n",
    "    end\n",
    "    subgraph Hidden_Layer_1[Layer 1]\n",
    "        Z1[z^[1]] -- g^[1] --> A1[a^[1]]\n",
    "        Cache1((cache^[1]))\n",
    "        A0 --> Z1\n",
    "        Z1 --> Cache1\n",
    "    end\n",
    "    subgraph Hidden_Layer_2[Layer 2]\n",
    "        Z2[z^[2]] -- g^[2] --> A2[a^[2]]\n",
    "        Cache2((cache^[2]))\n",
    "        A1 --> Z2\n",
    "        Z2 --> Cache2\n",
    "    end\n",
    "    subgraph Output_Layer[Output]\n",
    "        ZL[z^[L]] -- g^[L] --> AL[a^[L] = y^]\n",
    "        CacheL((cache^[L]))\n",
    "        A2 --> ZL\n",
    "        ZL --> CacheL\n",
    "    end\n",
    "\n",
    "    style Input_Layer fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style Output_Layer fill:#f9f,stroke:#333,stroke-width:2px\n",
    "\n",
    "    AL --- Loss(Loss Function)\n",
    "\n",
    "    subgraph Backprop_Output_Layer[Backward Layer L]\n",
    "        dAL[da^[L]]\n",
    "        dWL[dW^[L]]\n",
    "        dbL[db^[L]]\n",
    "        dZL[dz^[L]]\n",
    "        CacheL_B[cache^[L]]\n",
    "    end\n",
    "    subgraph Backprop_Hidden_Layer_2[Backward Layer 2]\n",
    "        dA2[da^[2]]\n",
    "        dW2[dW^[2]]\n",
    "        db2[db^[2]]\n",
    "        dZ2[dz^[2]]\n",
    "        Cache2_B[cache^[2]]\n",
    "    end\n",
    "    subgraph Backprop_Hidden_Layer_1[Backward Layer 1]\n",
    "        dA1[da^[1]]\n",
    "        dW1[dW^[1]]\n",
    "        db1[db^[1]]\n",
    "        dZ1[dz^[1]]\n",
    "        Cache1_B[cache^[1]]\n",
    "    end\n",
    "\n",
    "    Loss --> dAL\n",
    "    dAL --> dZL\n",
    "    CacheL --> CacheL_B\n",
    "    CacheL_B --> dZL\n",
    "    dZL --> dWL\n",
    "    dZL --> dbL\n",
    "    dZL --> dA2\n",
    "\n",
    "    dA2 --> dZ2\n",
    "    Cache2 --> Cache2_B\n",
    "    Cache2_B --> dZ2\n",
    "    dZ2 --> dW2\n",
    "    dZ2 --> db2\n",
    "    dZ2 --> dA1\n",
    "\n",
    "    dA1 --> dZ1\n",
    "    Cache1 --> Cache1_B\n",
    "    Cache1_B --> dZ1\n",
    "    dZ1 --> dW1\n",
    "    dZ1 --> db1\n",
    "\n",
    "    dWL --- Update_W_L(Update W^[L])\n",
    "    dbL --- Update_b_L(Update b^[L])\n",
    "    dW2 --- Update_W_2(Update W^[2])\n",
    "    db2 --- Update_b_2(Update b^[2])\n",
    "    dW1 --- Update_W_1(Update W^[1])\n",
    "    db1 --- Update_b_1(Update b^[1])\n",
    "```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d5349",
   "metadata": {},
   "source": [
    "## Important Formulas ### Forward Propagation (Vectorized) $$ Z^{[l]} = W^{[l]}A^{[l-1]} + B^{[l]} $$ $$ A^{[l]} = g^{[l]}(Z^{[l]}) $$ ### Matrix Dimensions - Weights: $$ ext{dim}(W^{[l]}) = (n^{[l]}, n^{[l-1]}) $$ - Biases: $$ ext{dim}(b^{[l]}) = (n^{[l]}, 1) $$ - Activations (vectorized, \\( m \\) examples): $$ ext{dim}(A^{[l]}) = (n^{[l]}, m) $$ - Linear combination (vectorized, \\( m \\) examples): $$ ext{dim}(Z^{[l]}) = (n^{[l]}, m) $$ ### Backpropagation (Vectorized) 1. $$ dZ^{[l]} = dA^{[l]} \\odot g^{'[l]}(Z^{[l]}) $$ 2. $$ dW^{[l]} = rac{1}{m} dZ^{[l]} (A^{[l-1]})^T $$ 3. $$ db^{[l]} = rac{1}{m} ext{np.sum}(dZ^{[l]}, ext{axis}=1, ext{keepdims}= ext{True}) $$ 4. $$ dA^{[l-1]} = (W^{[l]})^T dZ^{[l]} $$ ### Initializing \\( dA^{[L]} \\) for Binary Classification $$ dA^{[L]} = -rac{Y}{A^{[L]}} + rac{1-Y}{1-A^{[L]}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb9bd3c",
   "metadata": {},
   "source": [
    "## Practical Understanding Think of a deep neural network as an assembly line in a sophisticated factory. - **Input Layer**: The raw materials (data, e.g., image pixels) enter the factory. - **First Hidden Layer**: Performs initial processing, identifying simple components (e.g., edges, basic sounds). Each 'worker' (neuron) specializes in detecting one type of simple component. - **Subsequent Hidden Layers**: Intermediate workers assemble these simple components into more complex parts. For an image, edges become eyes or noses; for audio, basic sounds become phonemes. Each layer builds upon the output of the previous one, creating increasingly abstract representations. - **Output Layer**: The final assembly line produces the finished product (e.g., a recognized face, a transcribed word, a classification decision). - **Backpropagation**: If the final product isn't perfect, quality control (backpropagation) traces errors back through the assembly line, identifying which workers (weights and biases) need to adjust their techniques (gradient updates) to improve the output in the future. - **Hyperparameters**: These are the factory's strategic decisions, like the number of assembly stages (layers), the number of workers at each stage (hidden units), or the overall speed of the assembly line (learning rate). These need careful tuning to optimize the factory's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346a978",
   "metadata": {},
   "source": [
    "## Quick Revision - **Deep Network**: Multiple hidden layers for complex feature learning. - **Layer Count**: Hidden layers + output layer (input layer is 0). - **Notation**: \\( L \\) (total layers), \\( n^{[l]} \\) (units), \\( W^{[l]}, b^{[l]} \\) (params), \\( a^{[l]}, z^{[l]} \\) (activations). - **Forward Prop**: \\( Z^{[l]} = W^{[l]}A^{[l-1]} + B^{[l]} \\), \\( A^{[l]} = g^{[l]}(Z^{[l]}) \\). - **Matrix Dims**: \\( W^{[l]} \\) is \\( (n^{[l]}, n^{[l-1]}) \\), \\( B^{[l]} \\) is \\( (n^{[l]}, 1) \\), \\( A^{[l]}, Z^{[l]} \\) are \\( (n^{[l]}, m) \\) for vectorized. - **Why Deep?**: Hierarchical feature learning (simple to complex), efficiency for certain functions. - **Building Blocks**: Forward and backward functions per layer, using `cache` to pass values. - **Backprop Equations**: Specific formulas for \\( dZ^{[l]}, dW^{[l]}, db^{[l]}, dA^{[l-1]} \\). - **Initial \\( dA^{[L]} \\)**: Derived from the loss function (e.g., for binary classification). - **Parameters**: Learned (W, b). **Hyperparameters**: Tuned (\\( \u0007lpha \\), \\( L \\), \\( n^{[l]} \\), activation functions). - **Brain Analogy**: Loose and becoming less relevant, focus on functional capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5beec3f",
   "metadata": {},
   "source": [
    "## Practice Questions (Optional) 1. Explain the difference between a 2-layer neural network and a 4-layer neural network in terms of architecture and potential learning capacity. 2. Given a deep neural network with \\( L=3 \\) hidden layers, where \\( n^{[0]}=10 \\), \\( n^{[1]}=20 \\), \\( n^{[2]}=15 \\), \\( n^{[3]}=5 \\), and \\( n^{[4]}=1 \\), what are the dimensions of \\( W^{[2]} \\) and \\( b^{[3]} \\)? Assume \\( m=100 \\) training examples, what are the dimensions of \\( A^{[1]} \\) and \\( Z^{[4]} \\)? 3. Why is it acceptable to use a `for` loop to iterate through layers in forward and backward propagation, even though we generally strive for vectorization to avoid `for` loops in neural network implementations? 4. Describe the hierarchical feature learning concept. Provide an example other than image or speech recognition where deep representations could be beneficial. 5. What is the primary distinction between a 'parameter' and a 'hyperparameter' in the context of deep learning? Provide at least three examples of each. 6. You are debugging your deep learning model and notice that the dimensions of your \\( W^{[l]} \\) matrices are incorrect. Which specific equation or rule would you refer to first to correct this issue?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
